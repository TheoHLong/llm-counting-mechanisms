{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Dataset"
      ],
      "metadata": {
        "id": "7p0hStXlWzk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkvxaS_tlu3D",
        "outputId": "0fdce6f6-27f1-445e-92f3-f3e56a1edfdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word banks saved to: counting_dataset/word_banks.json\n",
            "Categories: 11\n",
            "Total category words: 534\n",
            "  fruit: 48 words\n",
            "  animal: 63 words\n",
            "  vehicle: 49 words\n",
            "  color: 46 words\n",
            "  body_part: 47 words\n",
            "  tool: 43 words\n",
            "  clothing: 46 words\n",
            "  sport: 47 words\n",
            "  building: 51 words\n",
            "  weather: 47 words\n",
            "  emotion: 47 words\n",
            "\n",
            "Generating dataset with UNIFORM distribution...\n",
            "\n",
            "=== Dataset Validation ===\n",
            "Total examples: 5000\n",
            "Answer distribution: {1: 600, 2: 628, 3: 602, 0: 651, 9: 163, 6: 452, 4: 622, 7: 374, 5: 613, 10: 64, 8: 231}\n",
            "Answer distribution (percentages):\n",
            "  0: 651 examples (13.0%)\n",
            "  1: 600 examples (12.0%)\n",
            "  2: 628 examples (12.6%)\n",
            "  3: 602 examples (12.0%)\n",
            "  4: 622 examples (12.4%)\n",
            "  5: 613 examples (12.3%)\n",
            "  6: 452 examples (9.0%)\n",
            "  7: 374 examples (7.5%)\n",
            "  8: 231 examples (4.6%)\n",
            "  9: 163 examples (3.3%)\n",
            "  10: 64 examples (1.3%)\n",
            "Category distribution: {'body_part': 487, 'building': 414, 'emotion': 449, 'vehicle': 503, 'animal': 477, 'sport': 418, 'weather': 478, 'color': 436, 'fruit': 435, 'clothing': 460, 'tool': 443}\n",
            "List length distribution: {6: 798, 5: 785, 7: 902, 9: 845, 8: 836, 10: 834}\n",
            "Duplicate examples: 0\n",
            "\n",
            "Dataset exported to: counting_dataset/counting_dataset_5000.csv\n",
            "Total examples: 5000\n",
            "Word banks saved to: counting_dataset/word_banks.json\n",
            "\n",
            "=== Sample Examples ===\n",
            "\n",
            "Example 1:\n",
            "Type: body_part\n",
            "List: 'optimistic', 'leg', 'khaki', 'billiards', 'tennis', 'greenhouse'\n",
            "Answer: 1\n",
            "\n",
            "Example 2:\n",
            "Type: building\n",
            "List: 'swimming', 'eye', 'cathedral', 'hippo', 'grapefruit'\n",
            "Answer: 1\n",
            "\n",
            "Example 3:\n",
            "Type: building\n",
            "List: 'university', 'shed', 'jaw', 'skirt', 'snow'\n",
            "Answer: 2\n",
            "\n",
            "=== Quick Usage Example ===\n",
            "import pandas as pd\n",
            "df = pd.read_csv('counting_dataset/counting_dataset_5000.csv')\n",
            "print(df.head())\n",
            "\n",
            "# Easy to work with - list_items are ready to use:\n",
            "word_list = eval('[' + df.iloc[0]['list_items'] + ']')\n",
            "# Or use ast.literal_eval for safety\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Comprehensive word banks - 11 categories with 30-80 words each\n",
        "word_banks = {\n",
        "    \"fruit\": [\n",
        "        \"apple\", \"banana\", \"cherry\", \"grape\", \"orange\", \"lemon\", \"peach\", \"pear\", \"plum\", \"berry\",\n",
        "        \"melon\", \"kiwi\", \"mango\", \"lime\", \"date\", \"fig\", \"apricot\", \"coconut\", \"papaya\", \"guava\",\n",
        "        \"pomegranate\", \"avocado\", \"strawberry\", \"blueberry\", \"raspberry\", \"blackberry\", \"cranberry\",\n",
        "        \"watermelon\", \"cantaloupe\", \"honeydew\", \"grapefruit\", \"tangerine\", \"nectarine\", \"persimmon\",\n",
        "        \"dragonfruit\", \"passionfruit\", \"starfruit\", \"lychee\", \"rambutan\", \"jackfruit\", \"durian\",\n",
        "        \"elderberry\", \"gooseberry\", \"currant\", \"mulberry\", \"boysenberry\", \"lingonberry\", \"cloudberry\"\n",
        "    ],\n",
        "\n",
        "    \"animal\": [\n",
        "        \"dog\", \"cat\", \"bird\", \"fish\", \"lion\", \"bear\", \"wolf\", \"deer\", \"mouse\", \"rabbit\",\n",
        "        \"horse\", \"cow\", \"pig\", \"sheep\", \"goat\", \"chicken\", \"duck\", \"goose\", \"turkey\", \"eagle\",\n",
        "        \"hawk\", \"owl\", \"parrot\", \"canary\", \"pigeon\", \"crow\", \"sparrow\", \"robin\", \"cardinal\",\n",
        "        \"tiger\", \"leopard\", \"cheetah\", \"jaguar\", \"panther\", \"elephant\", \"rhino\", \"hippo\", \"giraffe\",\n",
        "        \"zebra\", \"kangaroo\", \"koala\", \"panda\", \"monkey\", \"gorilla\", \"chimpanzee\", \"orangutan\",\n",
        "        \"whale\", \"dolphin\", \"shark\", \"octopus\", \"squid\", \"crab\", \"lobster\", \"shrimp\", \"jellyfish\",\n",
        "        \"turtle\", \"frog\", \"toad\", \"snake\", \"lizard\", \"crocodile\", \"alligator\", \"iguana\"\n",
        "    ],\n",
        "\n",
        "    \"vehicle\": [\n",
        "        \"car\", \"bus\", \"truck\", \"bike\", \"plane\", \"boat\", \"train\", \"taxi\", \"van\", \"ship\",\n",
        "        \"jet\", \"helicopter\", \"motorcycle\", \"scooter\", \"bicycle\", \"tricycle\", \"subway\", \"tram\",\n",
        "        \"ferry\", \"yacht\", \"canoe\", \"kayak\", \"sailboat\", \"speedboat\", \"cruise\", \"cargo\", \"tanker\",\n",
        "        \"ambulance\", \"firetruck\", \"police\", \"limousine\", \"convertible\", \"sedan\", \"hatchback\",\n",
        "        \"wagon\", \"pickup\", \"trailer\", \"semi\", \"bulldozer\", \"excavator\", \"crane\", \"forklift\",\n",
        "        \"tractor\", \"combine\", \"harvester\", \"snowplow\", \"garbage\", \"delivery\", \"rickshaw\"\n",
        "    ],\n",
        "\n",
        "    \"color\": [\n",
        "        \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \"pink\", \"brown\", \"gray\", \"purple\",\n",
        "        \"orange\", \"silver\", \"gold\", \"violet\", \"cyan\", \"magenta\", \"turquoise\", \"indigo\", \"maroon\",\n",
        "        \"navy\", \"teal\", \"lime\", \"olive\", \"aqua\", \"fuchsia\", \"coral\", \"salmon\", \"peach\", \"beige\",\n",
        "        \"tan\", \"khaki\", \"ivory\", \"cream\", \"pearl\", \"platinum\", \"bronze\", \"copper\", \"rust\",\n",
        "        \"crimson\", \"scarlet\", \"burgundy\", \"lavender\", \"lilac\", \"periwinkle\", \"azure\", \"cobalt\"\n",
        "    ],\n",
        "\n",
        "    \"body_part\": [\n",
        "        \"head\", \"face\", \"eye\", \"nose\", \"mouth\", \"ear\", \"neck\", \"shoulder\", \"arm\", \"elbow\",\n",
        "        \"wrist\", \"hand\", \"finger\", \"thumb\", \"nail\", \"chest\", \"back\", \"waist\", \"hip\", \"leg\",\n",
        "        \"thigh\", \"knee\", \"shin\", \"ankle\", \"foot\", \"toe\", \"heel\", \"brain\", \"heart\",\n",
        "        \"lung\", \"liver\", \"kidney\", \"stomach\", \"intestine\", \"muscle\", \"bone\", \"skin\", \"hair\",\n",
        "        \"eyebrow\", \"eyelash\", \"cheek\", \"chin\", \"forehead\", \"temple\", \"jaw\", \"tooth\", \"tongue\"\n",
        "    ],\n",
        "\n",
        "    \"tool\": [\n",
        "        \"hammer\", \"wrench\", \"screwdriver\", \"drill\", \"saw\", \"pliers\", \"knife\", \"scissors\", \"ruler\",\n",
        "        \"tape\", \"level\", \"square\", \"chisel\", \"file\", \"sandpaper\", \"clamp\", \"vise\", \"anvil\",\n",
        "        \"toolbox\", \"workbench\", \"ladder\", \"stepladder\", \"crowbar\", \"pickaxe\", \"shovel\", \"rake\",\n",
        "        \"hoe\", \"spade\", \"trowel\", \"pruner\", \"shears\", \"mower\", \"trimmer\", \"blower\", \"chainsaw\",\n",
        "        \"welder\", \"grinder\", \"router\", \"jigsaw\", \"bandsaw\", \"lathe\", \"press\", \"compressor\"\n",
        "    ],\n",
        "\n",
        "    \"clothing\": [\n",
        "        \"shirt\", \"pants\", \"dress\", \"skirt\", \"jacket\", \"coat\", \"sweater\", \"hoodie\", \"blouse\", \"top\",\n",
        "        \"jeans\", \"shorts\", \"trousers\", \"suit\", \"tie\", \"scarf\", \"hat\", \"cap\", \"gloves\", \"socks\",\n",
        "        \"shoes\", \"boots\", \"sandals\", \"sneakers\", \"heels\", \"flats\", \"loafers\", \"slippers\", \"belt\",\n",
        "        \"vest\", \"cardigan\", \"blazer\", \"tuxedo\", \"gown\", \"robe\", \"pajamas\", \"underwear\", \"bra\",\n",
        "        \"bikini\", \"swimsuit\", \"uniform\", \"overalls\", \"jumpsuit\", \"romper\", \"tunic\", \"poncho\"\n",
        "    ],\n",
        "\n",
        "    \"sport\": [\n",
        "        \"football\", \"basketball\", \"baseball\", \"soccer\", \"tennis\", \"golf\", \"hockey\", \"volleyball\",\n",
        "        \"cricket\", \"rugby\", \"boxing\", \"wrestling\", \"swimming\", \"diving\", \"track\", \"marathon\",\n",
        "        \"cycling\", \"skiing\", \"snowboard\", \"surfing\", \"skateboard\", \"bowling\", \"billiards\", \"darts\",\n",
        "        \"archery\", \"fencing\", \"karate\", \"judo\", \"taekwondo\", \"gymnastics\", \"cheerleading\", \"dance\",\n",
        "        \"yoga\", \"pilates\", \"aerobics\", \"crossfit\", \"weightlifting\", \"powerlifting\", \"bodybuilding\",\n",
        "        \"climbing\", \"hiking\", \"camping\", \"fishing\", \"hunting\", \"sailing\", \"rowing\", \"kayaking\"\n",
        "    ],\n",
        "\n",
        "    \"building\": [\n",
        "        \"house\", \"apartment\", \"mansion\", \"cottage\", \"cabin\", \"castle\", \"palace\", \"tower\", \"skyscraper\",\n",
        "        \"office\", \"store\", \"shop\", \"mall\", \"market\", \"restaurant\", \"cafe\", \"bar\", \"hotel\", \"motel\",\n",
        "        \"hospital\", \"clinic\", \"school\", \"university\", \"library\", \"museum\", \"theater\", \"cinema\",\n",
        "        \"church\", \"temple\", \"mosque\", \"synagogue\", \"cathedral\", \"chapel\", \"monastery\", \"convent\",\n",
        "        \"factory\", \"warehouse\", \"garage\", \"barn\", \"shed\", \"greenhouse\", \"lighthouse\", \"windmill\",\n",
        "        \"bridge\", \"tunnel\", \"dam\", \"fort\", \"bunker\", \"observatory\", \"planetarium\", \"aquarium\"\n",
        "    ],\n",
        "\n",
        "    \"weather\": [\n",
        "        \"sun\", \"rain\", \"snow\", \"wind\", \"cloud\", \"storm\", \"thunder\", \"lightning\", \"hail\", \"sleet\",\n",
        "        \"fog\", \"mist\", \"drizzle\", \"shower\", \"downpour\", \"blizzard\", \"tornado\", \"hurricane\", \"cyclone\",\n",
        "        \"typhoon\", \"drought\", \"flood\", \"frost\", \"ice\", \"dew\", \"humidity\", \"pressure\", \"temperature\",\n",
        "        \"heat\", \"cold\", \"warm\", \"cool\", \"hot\", \"freezing\", \"mild\", \"severe\", \"gentle\", \"fierce\",\n",
        "        \"calm\", \"breezy\", \"gusty\", \"windy\", \"stormy\", \"sunny\", \"cloudy\", \"overcast\", \"clear\"\n",
        "    ],\n",
        "\n",
        "    \"emotion\": [\n",
        "        \"happy\", \"sad\", \"angry\", \"excited\", \"nervous\", \"calm\", \"peaceful\", \"anxious\", \"worried\",\n",
        "        \"scared\", \"afraid\", \"brave\", \"confident\", \"shy\", \"proud\", \"ashamed\", \"guilty\", \"innocent\",\n",
        "        \"curious\", \"bored\", \"interested\", \"fascinated\", \"amazed\", \"surprised\", \"shocked\", \"confused\",\n",
        "        \"frustrated\", \"annoyed\", \"irritated\", \"pleased\", \"satisfied\", \"content\", \"grateful\",\n",
        "        \"thankful\", \"hopeful\", \"optimistic\", \"pessimistic\", \"depressed\", \"elated\", \"ecstatic\",\n",
        "        \"enthusiastic\", \"passionate\", \"loving\", \"caring\", \"compassionate\", \"empathetic\", \"sympathetic\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def save_word_banks(output_dir=\"counting_dataset\"):\n",
        "    \"\"\"Save word banks to JSON file in specified directory\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    filepath = os.path.join(output_dir, 'word_banks.json')\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(word_banks, f, indent=2)\n",
        "\n",
        "    print(f\"Word banks saved to: {filepath}\")\n",
        "    print(f\"Categories: {len(word_banks)}\")\n",
        "    print(f\"Total category words: {sum(len(words) for words in word_banks.values())}\")\n",
        "\n",
        "    # Print category summary\n",
        "    for category, words in word_banks.items():\n",
        "        print(f\"  {category}: {len(words)} words\")\n",
        "\n",
        "def generate_example(category, list_length=7):\n",
        "    \"\"\"Generate a single counting example with UNIFORM distribution\"\"\"\n",
        "    target_words = word_banks[category]\n",
        "\n",
        "    # UNIFORM METHOD: Equal probability for each possible count\n",
        "    max_matches = min(len(target_words), list_length)\n",
        "    num_matches = random.randint(0, max_matches)\n",
        "\n",
        "    # Select matches from target category\n",
        "    matches = random.sample(target_words, num_matches) if num_matches > 0 else []\n",
        "\n",
        "    # Fill remaining slots with words from other categories\n",
        "    remaining_slots = list_length - len(matches)\n",
        "\n",
        "    fillers = []\n",
        "    other_categories = [cat for cat in word_banks if cat != category]\n",
        "\n",
        "    # Randomly sample from other categories to fill remaining slots\n",
        "    for _ in range(remaining_slots):\n",
        "        other_cat = random.choice(other_categories)\n",
        "        other_word = random.choice(word_banks[other_cat])\n",
        "        fillers.append(other_word)\n",
        "\n",
        "    # Combine and shuffle\n",
        "    word_list = matches + fillers\n",
        "    random.shuffle(word_list)\n",
        "\n",
        "    return {\n",
        "        'type': category,\n",
        "        'list_items': ', '.join([f\"'{word}'\" for word in word_list]),  # Proper Python list format\n",
        "        'list_length': len(word_list),\n",
        "        'answer': len(matches)\n",
        "    }\n",
        "\n",
        "def create_dataset(target_size=5000):\n",
        "    \"\"\"Generate complete dataset\"\"\"\n",
        "    examples = []\n",
        "    categories_list = list(word_banks.keys())\n",
        "    list_lengths = [5, 6, 7, 8, 9, 10]  # Variable list lengths\n",
        "\n",
        "    for i in range(target_size):\n",
        "        category = random.choice(categories_list)\n",
        "        list_length = random.choice(list_lengths)\n",
        "\n",
        "        try:\n",
        "            example = generate_example(category, list_length)\n",
        "            examples.append(example)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating example {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return examples\n",
        "\n",
        "def validate_dataset(dataset):\n",
        "    \"\"\"Validate dataset quality\"\"\"\n",
        "    print(f\"\\n=== Dataset Validation ===\")\n",
        "    print(f\"Total examples: {len(dataset)}\")\n",
        "\n",
        "    # Answer distribution\n",
        "    answer_dist = Counter([ex['answer'] for ex in dataset])\n",
        "    print(f\"Answer distribution: {dict(answer_dist)}\")\n",
        "\n",
        "    # Show percentages for better understanding\n",
        "    total = len(dataset)\n",
        "    print(\"Answer distribution (percentages):\")\n",
        "    for answer in sorted(answer_dist.keys()):\n",
        "        count = answer_dist[answer]\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"  {answer}: {count} examples ({percentage:.1f}%)\")\n",
        "\n",
        "    # Category distribution\n",
        "    category_dist = Counter([ex['type'] for ex in dataset])\n",
        "    print(f\"Category distribution: {dict(category_dist)}\")\n",
        "\n",
        "    # List length distribution\n",
        "    length_dist = Counter([ex['list_length'] for ex in dataset])\n",
        "    print(f\"List length distribution: {dict(length_dist)}\")\n",
        "\n",
        "    # Check for duplicates based on type + list_items combination\n",
        "    examples_key = [(ex['type'], ex['list_items']) for ex in dataset]\n",
        "    duplicates = len(examples_key) - len(set(examples_key))\n",
        "    print(f\"Duplicate examples: {duplicates}\")\n",
        "\n",
        "    return len(dataset) - duplicates\n",
        "\n",
        "def export_dataset(dataset, output_dir=\"counting_dataset\"):\n",
        "    \"\"\"Export dataset as single CSV file\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Define CSV fieldnames - minimal columns for data generation\n",
        "    fieldnames = ['type', 'list_items', 'list_length', 'answer']\n",
        "\n",
        "    # Export main dataset\n",
        "    dataset_path = os.path.join(output_dir, f'counting_dataset_{len(dataset)}.csv')\n",
        "    with open(dataset_path, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(dataset)\n",
        "\n",
        "    print(f\"\\nDataset exported to: {dataset_path}\")\n",
        "    print(f\"Total examples: {len(dataset)}\")\n",
        "    print(f\"Word banks saved to: {os.path.join(output_dir, 'word_banks.json')}\")\n",
        "\n",
        "    return dataset_path\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Set output directory\n",
        "    output_directory = \"counting_dataset\"\n",
        "\n",
        "    # Save word banks\n",
        "    save_word_banks(output_directory)\n",
        "\n",
        "    # Generate dataset\n",
        "    print(\"\\nGenerating dataset with UNIFORM distribution...\")\n",
        "    dataset = create_dataset(5000)\n",
        "\n",
        "    # Validate\n",
        "    valid_count = validate_dataset(dataset)\n",
        "\n",
        "    # Export\n",
        "    dataset_path = export_dataset(dataset, output_dir=output_directory)\n",
        "\n",
        "    # Show sample examples\n",
        "    print(f\"\\n=== Sample Examples ===\")\n",
        "    for i in range(3):\n",
        "        ex = dataset[i]\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Type: {ex['type']}\")\n",
        "        print(f\"List: {ex['list_items']}\")\n",
        "        print(f\"Answer: {ex['answer']}\")\n",
        "\n",
        "    # Show CSV loading example\n",
        "    print(f\"\\n=== Quick Usage Example ===\")\n",
        "    print(f\"import pandas as pd\")\n",
        "    print(f\"df = pd.read_csv('{dataset_path}')\")\n",
        "    print(f\"print(df.head())\")\n",
        "    print(f\"\\n# Easy to work with - list_items are ready to use:\")\n",
        "    print(f\"word_list = eval('[' + df.iloc[0]['list_items'] + ']')\")\n",
        "    print(f\"# Or use ast.literal_eval for safety\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src_dir = '/content/counting_dataset'\n",
        "dst_dir = '/content/drive/MyDrive/counting_project/counting_dataset'\n",
        "\n",
        "# Make sure destination directory exists\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files in source directory\n",
        "for filename in os.listdir(src_dir):\n",
        "    src_file = os.path.join(src_dir, filename)\n",
        "    dst_file = os.path.join(dst_dir, filename)\n",
        "\n",
        "    # Only copy files (skip subdirectories)\n",
        "    if os.path.isfile(src_file):\n",
        "        shutil.copy2(src_file, dst_file)  # copy2 preserves metadata"
      ],
      "metadata": {
        "id": "WtoVSK9Tmj1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Mediator Dataset"
      ],
      "metadata": {
        "id": "qqx6PkzKW3DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "class CMAWordCountDataGenerator:\n",
        "    def __init__(self, word_banks_path: str = \"word_banks.json\"):\n",
        "        \"\"\"Initialize with word banks\"\"\"\n",
        "        with open(word_banks_path, 'r') as f:\n",
        "            self.word_banks = json.load(f)\n",
        "\n",
        "        self.categories = list(self.word_banks.keys())\n",
        "        print(f\"Loaded {len(self.categories)} categories: {self.categories}\")\n",
        "\n",
        "    def create_base_example(self, target_category: str, list_length: int = 7) -> Dict:\n",
        "        \"\"\"Create a single base example for counting words of a specific category\"\"\"\n",
        "        # Number of target category words (1 to list_length-2)\n",
        "        num_target_words = random.randint(1, max(2, list_length - 2))\n",
        "\n",
        "        # Sample words from target category\n",
        "        target_words = random.sample(self.word_banks[target_category],\n",
        "                                   min(num_target_words, len(self.word_banks[target_category])))\n",
        "\n",
        "        # Sample distractor words from other categories\n",
        "        distractor_words = []\n",
        "        other_categories = [cat for cat in self.categories if cat != target_category]\n",
        "\n",
        "        while len(distractor_words) < list_length - len(target_words):\n",
        "            random_category = random.choice(other_categories)\n",
        "            word = random.choice(self.word_banks[random_category])\n",
        "            if word not in distractor_words and word not in target_words:\n",
        "                distractor_words.append(word)\n",
        "\n",
        "        # Create word list and shuffle\n",
        "        word_list = target_words + distractor_words\n",
        "        random.shuffle(word_list)\n",
        "\n",
        "        # Record positions of target words\n",
        "        target_positions = [i for i, word in enumerate(word_list)\n",
        "                          if word in target_words]\n",
        "\n",
        "        return {\n",
        "            'category': target_category,\n",
        "            'word_list': word_list,\n",
        "            'target_words': target_words,\n",
        "            'distractor_words': distractor_words,\n",
        "            'target_positions': target_positions,\n",
        "            'count': len(target_words),\n",
        "            'list_length': list_length\n",
        "        }\n",
        "\n",
        "    def create_intervention_pair(self, base_example: Dict) -> Dict:\n",
        "        \"\"\"Create an intervention by replacing a target word with a distractor\"\"\"\n",
        "        target_positions = base_example['target_positions']\n",
        "\n",
        "        if not target_positions:\n",
        "            return None\n",
        "\n",
        "        # Choose a random target word position to intervene\n",
        "        intervention_pos = random.choice(target_positions)\n",
        "        original_word = base_example['word_list'][intervention_pos]\n",
        "\n",
        "        # Choose a replacement word from a different category\n",
        "        other_categories = [cat for cat in self.categories if cat != base_example['category']]\n",
        "        replacement_category = random.choice(other_categories)\n",
        "\n",
        "        # Get a word that's not already in the list\n",
        "        available_words = [w for w in self.word_banks[replacement_category]\n",
        "                          if w not in base_example['word_list']]\n",
        "\n",
        "        if not available_words:\n",
        "            # Try another category\n",
        "            for cat in other_categories:\n",
        "                available_words = [w for w in self.word_banks[cat]\n",
        "                                 if w not in base_example['word_list']]\n",
        "                if available_words:\n",
        "                    replacement_category = cat\n",
        "                    break\n",
        "\n",
        "        if not available_words:\n",
        "            return None\n",
        "\n",
        "        intervention_word = random.choice(available_words)\n",
        "\n",
        "        # Create intervention list\n",
        "        intervention_list = base_example['word_list'].copy()\n",
        "        intervention_list[intervention_pos] = intervention_word\n",
        "\n",
        "        return {\n",
        "            'category': base_example['category'],\n",
        "            'original_list': base_example['word_list'],\n",
        "            'intervention_list': intervention_list,\n",
        "            'original_count': base_example['count'],\n",
        "            'intervention_count': base_example['count'] - 1,  # One less target word\n",
        "            'intervention_position': intervention_pos,\n",
        "            'original_word': original_word,\n",
        "            'intervention_word': intervention_word,\n",
        "            'intervention_category': replacement_category\n",
        "        }\n",
        "\n",
        "    def generate_dataset_pairs(self, num_pairs: int = 3000, output_dir: str = \"/content/counting_dataset\") -> List[Dict]:\n",
        "        \"\"\"Generate intervention pairs and save to specified directory\"\"\"\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        intervention_pairs = []\n",
        "        attempts = 0\n",
        "        max_attempts = num_pairs * 3  # Allow more attempts to reach target\n",
        "\n",
        "        while len(intervention_pairs) < num_pairs and attempts < max_attempts:\n",
        "            attempts += 1\n",
        "\n",
        "            # Create a base example\n",
        "            category = random.choice(self.categories)\n",
        "            list_length = random.randint(5, 8)\n",
        "            base_example = self.create_base_example(category, list_length)\n",
        "\n",
        "            # Create intervention pair\n",
        "            intervention = self.create_intervention_pair(base_example)\n",
        "            if intervention:\n",
        "                intervention['pair_id'] = len(intervention_pairs)\n",
        "                intervention_pairs.append(intervention)\n",
        "\n",
        "        # Save the pairs\n",
        "        output_path = os.path.join(output_dir, \"cma_intervention_pairs.json\")\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(intervention_pairs, f, indent=2)\n",
        "\n",
        "        print(f\"\\nGenerated {len(intervention_pairs)} intervention pairs\")\n",
        "        print(f\"Saved to: {output_path}\")\n",
        "\n",
        "        # Print basic statistics\n",
        "        self.print_statistics(intervention_pairs)\n",
        "\n",
        "        return intervention_pairs\n",
        "\n",
        "    def print_statistics(self, intervention_pairs: List[Dict]):\n",
        "        \"\"\"Print dataset statistics\"\"\"\n",
        "        print(\"\\n=== Dataset Statistics ===\")\n",
        "        print(f\"Total intervention pairs: {len(intervention_pairs)}\")\n",
        "\n",
        "        # Category distribution\n",
        "        category_counts = defaultdict(int)\n",
        "        position_dist = defaultdict(int)\n",
        "\n",
        "        for pair in intervention_pairs:\n",
        "            category_counts[pair['category']] += 1\n",
        "            position_dist[pair['intervention_position']] += 1\n",
        "\n",
        "        print(\"\\nCategory distribution:\")\n",
        "        for cat, count in sorted(category_counts.items()):\n",
        "            print(f\"  {cat}: {count}\")\n",
        "\n",
        "        print(\"\\nIntervention position distribution:\")\n",
        "        for pos, count in sorted(position_dist.items()):\n",
        "            print(f\"  Position {pos}: {count}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize generator with your path\n",
        "    generator = CMAWordCountDataGenerator(\"/content/counting_dataset/word_banks.json\")\n",
        "\n",
        "    # Generate dataset pairs and save to specified directory\n",
        "    pairs = generator.generate_dataset_pairs(num_pairs=3000, output_dir=\"/content/CMA_analysis\")\n",
        "\n",
        "    # Show a few examples\n",
        "    print(\"\\n=== Example Pairs ===\")\n",
        "    for i in range(min(3, len(pairs))):\n",
        "        pair = pairs[i]\n",
        "        print(f\"\\nPair {pair['pair_id']}:\")\n",
        "        print(f\"  Category: {pair['category']}\")\n",
        "        print(f\"  Original: {pair['original_list']} (count: {pair['original_count']})\")\n",
        "        print(f\"  Intervention: {pair['intervention_list']} (count: {pair['intervention_count']})\")\n",
        "        print(f\"  Changed: '{pair['original_word']}' -> '{pair['intervention_word']}' at position {pair['intervention_position']}\")"
      ],
      "metadata": {
        "id": "rn3hc7WJXBey"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}