{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc28870141164574baafd4efacd9cea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b2eabe0b5594a7e87fab9d6ce270ea4",
              "IPY_MODEL_6488063d7f6a4c689260f7a2fb0776c3",
              "IPY_MODEL_a4ebe1718d1d4816ad323b223d56df37"
            ],
            "layout": "IPY_MODEL_ca093a377c484b13b0f186908e874761"
          }
        },
        "9b2eabe0b5594a7e87fab9d6ce270ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7641e2a43e85499684f1b56de9470024",
            "placeholder": "​",
            "style": "IPY_MODEL_518294421eeb426f9ea4e0235d4f8246",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6488063d7f6a4c689260f7a2fb0776c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d18d3afb76a64110ac62a1906f57a147",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5118fed731204e17b14dc40345e5bbc9",
            "value": 6
          }
        },
        "a4ebe1718d1d4816ad323b223d56df37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32d7273adf74d8ea476783ccb4cb56a",
            "placeholder": "​",
            "style": "IPY_MODEL_d4674610e74949b4be3e8938ebcf15cf",
            "value": " 6/6 [00:08&lt;00:00,  1.39s/it]"
          }
        },
        "ca093a377c484b13b0f186908e874761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7641e2a43e85499684f1b56de9470024": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "518294421eeb426f9ea4e0235d4f8246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d18d3afb76a64110ac62a1906f57a147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5118fed731204e17b14dc40345e5bbc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e32d7273adf74d8ea476783ccb4cb56a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4674610e74949b4be3e8938ebcf15cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "lcUJBFK3qT6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src_dir = '/content/drive/MyDrive/counting_project/counting_dataset/CMA_analysis'\n",
        "dst_dir = '/content/CMA_analysis'\n",
        "\n",
        "# Make sure destination directory exists\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files in source directory\n",
        "for filename in os.listdir(src_dir):\n",
        "    src_file = os.path.join(src_dir, filename)\n",
        "    dst_file = os.path.join(dst_dir, filename)\n",
        "\n",
        "    # Only copy files (skip subdirectories)\n",
        "    if os.path.isfile(src_file):\n",
        "        shutil.copy2(src_file, dst_file)  # copy2 preserves metadata"
      ],
      "metadata": {
        "id": "MGfWNcncqdAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# counterfactual activation patching"
      ],
      "metadata": {
        "id": "m0wcwzULqdyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calculating TE,IE"
      ],
      "metadata": {
        "id": "zraOyMVZG88p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import gc\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class CMAEffectsCalculator:\n",
        "    def __init__(self, model_name: str = \"microsoft/phi-4\", device: Optional[str] = None):\n",
        "        \"\"\"Initialize model and tokenizer for effects calculation\"\"\"\n",
        "        print(f\"Loading model {model_name}...\")\n",
        "\n",
        "        if device is None:\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Load tokenizer and model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        self.model.eval()\n",
        "\n",
        "        # Get model info\n",
        "        if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
        "            self.num_layers = len(self.model.model.layers)\n",
        "            self.layer_attr = 'model.layers'\n",
        "        elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n",
        "            self.num_layers = len(self.model.transformer.h)\n",
        "            self.layer_attr = 'transformer.h'\n",
        "        else:\n",
        "            raise ValueError(\"Cannot determine model architecture\")\n",
        "\n",
        "        print(f\"Model loaded with {self.num_layers} layers\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "\n",
        "    def format_prompt(self, category: str, word_list: List[str]) -> str:\n",
        "        \"\"\"Format the counting prompt to match your evaluation script exactly\"\"\"\n",
        "        # Convert list to string format matching your examples\n",
        "        word_list_str = ' '.join(word_list)\n",
        "\n",
        "        # Match the exact format from your evaluation script\n",
        "        problem = f\"\"\"Count how many words in this list match the type \"{category}\".\n",
        "\n",
        "List: {word_list}\n",
        "\n",
        "Respond with only the number in parentheses, like (0), (1), (2), etc.\"\"\"\n",
        "\n",
        "        # Use the same message format\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise counting assistant. When given a list and a type, count how many items match that type. Always respond with ONLY the count in parentheses format: (0), (1), (2), etc. Never include explanations or other text.\"},\n",
        "            {\"role\": \"user\", \"content\": problem}\n",
        "        ]\n",
        "\n",
        "        return self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "    def extract_number_from_output(self, text: str) -> int:\n",
        "        \"\"\"Extract number from model output - matching your evaluation script exactly\"\"\"\n",
        "        # Clean the text\n",
        "        text = text.strip()\n",
        "\n",
        "        # Look for exact pattern (number)\n",
        "        match = re.search(r'\\((\\d+)\\)', text)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "\n",
        "        # If no parentheses, look for just a number\n",
        "        match = re.search(r'^\\s*(\\d+)\\s*$', text)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "\n",
        "        # If still nothing, look for any number in the text\n",
        "        match = re.search(r'(\\d+)', text)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "\n",
        "        return -1  # Return -1 if no answer found\n",
        "\n",
        "    def get_model_output(self, prompt: str) -> Tuple[int, str, torch.Tensor]:\n",
        "        \"\"\"Get model's prediction and logits\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=5,  # Match your evaluation script\n",
        "                # temperature=0.0,   # Deterministic\n",
        "                do_sample=False,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True\n",
        "            )\n",
        "\n",
        "        # Extract generated text\n",
        "        generated_ids = outputs.sequences[0][inputs.input_ids.shape[1]:]\n",
        "        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Extract number\n",
        "        predicted_number = self.extract_number_from_output(generated_text)\n",
        "\n",
        "        # Get last logits for probability analysis\n",
        "        if outputs.scores:\n",
        "            last_logits = outputs.scores[-1][0]\n",
        "        else:\n",
        "            last_logits = None\n",
        "\n",
        "        return predicted_number, generated_text, last_logits\n",
        "\n",
        "    def get_hidden_states(self, prompt: str) -> Tuple[Dict[int, torch.Tensor], torch.Tensor]:\n",
        "        \"\"\"Extract hidden states from all layers\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                **inputs,\n",
        "                output_hidden_states=True,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "        # Extract hidden states\n",
        "        hidden_states = {}\n",
        "        all_hidden = outputs.hidden_states\n",
        "\n",
        "        # Skip embedding layer (index 0)\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            hidden_states[layer_idx] = all_hidden[layer_idx + 1].cpu()\n",
        "\n",
        "        return hidden_states, inputs.input_ids\n",
        "\n",
        "    def find_intervention_token_positions(self, prompt: str, word_list: List[str],\n",
        "                                          intervention_position: int,\n",
        "                                          target_word: str) -> List[int]:\n",
        "        \"\"\"\n",
        "        Hybrid version:\n",
        "        1. Try to locate the token positions of `target_word` using intervention_position within `word_list`.\n",
        "        2. If that fails, fall back to robust token matching within the whole prompt.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        full_tokens = self.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "\n",
        "        # --- Primary strategy: Use intervention_position ---\n",
        "        if 0 <= intervention_position < len(word_list):\n",
        "            # Insert a space before the word to encourage separate tokenization\n",
        "            tokenized_word = self.tokenizer.tokenize(f\" {word_list[intervention_position]}\")\n",
        "\n",
        "            # Tokenize the full word list and match normalized version in full prompt\n",
        "            word_list_str = ' '.join(word_list)\n",
        "            word_list_tokens = self.tokenizer.tokenize(word_list_str)\n",
        "\n",
        "            # Locate word_list in full_tokens\n",
        "            for i in range(len(full_tokens) - len(word_list_tokens) + 1):\n",
        "                full_slice = full_tokens[i:i+len(word_list_tokens)]\n",
        "                if full_slice == word_list_tokens:\n",
        "                    start = i\n",
        "                    return list(range(start + sum(len(self.tokenizer.tokenize(f\" {word_list[j]}\"))\n",
        "                                                  for j in range(intervention_position)),\n",
        "                                      start + sum(len(self.tokenizer.tokenize(f\" {word_list[j]}\"))\n",
        "                                                  for j in range(intervention_position + 1))))\n",
        "\n",
        "        # --- Fallback strategy: Match target_word robustly ---\n",
        "        # Method 1: Try to find exact target_word tokens in the full prompt\n",
        "        target_word_tokens = self.tokenizer.tokenize(target_word)\n",
        "        target_word_tokens_normalized = [token.lstrip('Ġ▁') for token in target_word_tokens]\n",
        "\n",
        "        for i in range(len(full_tokens) - len(target_word_tokens) + 1):\n",
        "            slice_tokens = full_tokens[i:i+len(target_word_tokens)]\n",
        "            normalized = [t.lstrip('Ġ▁') for t in slice_tokens]\n",
        "            if normalized == target_word_tokens_normalized:\n",
        "                return list(range(i, i + len(target_word_tokens)))\n",
        "\n",
        "        # Method 2: Try fuzzy token string matching\n",
        "        variants = [\n",
        "            target_word.lower(),\n",
        "            f\"▁{target_word.lower()}\",\n",
        "            f\"Ġ{target_word.lower()}\",\n",
        "            target_word.lower().capitalize(),\n",
        "            f\"▁{target_word.lower().capitalize()}\",\n",
        "            f\"Ġ{target_word.lower().capitalize()}\"\n",
        "        ]\n",
        "\n",
        "        for i, token in enumerate(full_tokens):\n",
        "            cleaned = token.lower().lstrip(\"Ġ▁\")\n",
        "            if cleaned in [v.lstrip(\"Ġ▁\") for v in variants]:\n",
        "                return [i]\n",
        "\n",
        "        # If all fails, return empty\n",
        "        return []\n",
        "\n",
        "    def patch_forward_pass(self, prompt: str, layer_idx: int,\n",
        "                          patch_activation: torch.Tensor,\n",
        "                          patch_positions: List[int]) -> Tuple[int, str]:\n",
        "        \"\"\"Run forward pass with patched activations\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Get the layer to patch\n",
        "        if 'model.layers' in self.layer_attr:\n",
        "            target_layer = self.model.model.layers[layer_idx]\n",
        "        else:\n",
        "            target_layer = self.model.transformer.h[layer_idx]\n",
        "\n",
        "        # Store original forward method\n",
        "        original_forward = target_layer.forward\n",
        "\n",
        "        # Track if patch was applied\n",
        "        patch_applied = [False]\n",
        "\n",
        "        def patched_forward(hidden_states, *args, **kwargs):\n",
        "            # Call original forward\n",
        "            outputs = original_forward(hidden_states, *args, **kwargs)\n",
        "\n",
        "            # Extract hidden states from output\n",
        "            if isinstance(outputs, tuple):\n",
        "                hidden_states_out = outputs[0]\n",
        "                other_outputs = outputs[1:]\n",
        "            else:\n",
        "                hidden_states_out = outputs\n",
        "                other_outputs = ()\n",
        "\n",
        "            # Apply patch only at specified positions\n",
        "            for pos in patch_positions:\n",
        "                if pos < hidden_states_out.shape[1] and pos < patch_activation.shape[1]:\n",
        "                    # Apply patch\n",
        "                    patch_value = patch_activation[:, pos, :].to(hidden_states_out.device)\n",
        "                    hidden_states_out[:, pos, :] = patch_value\n",
        "                    patch_applied[0] = True\n",
        "\n",
        "            # Return in original format\n",
        "            if isinstance(outputs, tuple):\n",
        "                return (hidden_states_out,) + other_outputs\n",
        "            else:\n",
        "                return hidden_states_out\n",
        "\n",
        "        # Temporarily replace forward method\n",
        "        target_layer.forward = patched_forward\n",
        "\n",
        "        try:\n",
        "            # Generate with patched activation\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=5,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Extract result\n",
        "            generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
        "            generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "            predicted_number = self.extract_number_from_output(generated_text)\n",
        "\n",
        "        finally:\n",
        "            # Restore original forward method\n",
        "            target_layer.forward = original_forward\n",
        "\n",
        "        return predicted_number, generated_text\n",
        "\n",
        "    def calculate_effects(self, pair: Dict, layer_idx: int) -> Dict[str, Any]:\n",
        "        \"\"\"Calculate only TE and IE for a single pair and layer\"\"\"\n",
        "\n",
        "        # Format prompts\n",
        "        original_prompt = self.format_prompt(pair['category'], pair['original_list'])\n",
        "        intervention_prompt = self.format_prompt(pair['category'], pair['intervention_list'])\n",
        "\n",
        "        # Step 1: Get original and intervention outputs (for TE)\n",
        "        original_num, original_text, _ = self.get_model_output(original_prompt)\n",
        "        intervention_num, intervention_text, _ = self.get_model_output(intervention_prompt)\n",
        "\n",
        "        # Total Effect\n",
        "        TE = intervention_num - original_num\n",
        "\n",
        "        # Step 2: Get hidden states\n",
        "        original_hidden, _ = self.get_hidden_states(original_prompt)\n",
        "        intervention_hidden, _ = self.get_hidden_states(intervention_prompt)\n",
        "\n",
        "        # Step 3: Find positions to patch\n",
        "        patch_positions = self.find_intervention_token_positions(\n",
        "            intervention_prompt,\n",
        "            pair['intervention_list'],\n",
        "            pair['intervention_position'],\n",
        "            pair['intervention_word']\n",
        "        )\n",
        "\n",
        "        # Step 4: Calculate Indirect Effect (IE)\n",
        "        # IE: Run original prompt but patch intervention activations at original word position\n",
        "        ie_original_positions = self.find_intervention_token_positions(\n",
        "            original_prompt,\n",
        "            pair['original_list'],\n",
        "            pair['intervention_position'],\n",
        "            pair['original_word']\n",
        "        )\n",
        "\n",
        "        ie_num, ie_text = self.patch_forward_pass(\n",
        "            original_prompt,\n",
        "            layer_idx,\n",
        "            intervention_hidden[layer_idx],\n",
        "            ie_original_positions\n",
        "        )\n",
        "        IE = ie_num - original_num\n",
        "\n",
        "        # Collect results\n",
        "        return {\n",
        "            'pair_id': pair['pair_id'],\n",
        "            'layer_idx': layer_idx,\n",
        "            'TE': TE,\n",
        "            'IE': IE,\n",
        "            'original_output': original_num,\n",
        "            'intervention_output': intervention_num,\n",
        "            'ie_output': ie_num,\n",
        "            'original_text': original_text.strip(),\n",
        "            'intervention_text': intervention_text.strip(),\n",
        "            'ie_text': ie_text.strip(),\n",
        "            'expected_original': pair['original_count'],\n",
        "            'expected_intervention': pair['intervention_count'],\n",
        "            'original_correct': original_num == pair['original_count'],\n",
        "            'intervention_correct': intervention_num == pair['intervention_count'],\n",
        "            'patch_positions': patch_positions,\n",
        "            'ie_positions': ie_original_positions,\n",
        "            'num_patch_positions': len(patch_positions),\n",
        "            'num_ie_positions': len(ie_original_positions)\n",
        "        }\n",
        "\n",
        "    def calculate_effects_batch(self, pairs: List[Dict],\n",
        "                               layers_to_test: Optional[List[int]] = None,\n",
        "                               save_frequency: int = 10,\n",
        "                               output_dir: str = \"/content/counting_dataset\") -> pd.DataFrame:\n",
        "        \"\"\"Calculate effects for multiple pairs and layers\"\"\"\n",
        "\n",
        "        if layers_to_test is None:\n",
        "            # Test middle and later layers by default\n",
        "            layers_to_test = list(range(self.num_layers // 2, self.num_layers))\n",
        "            print(f\"Testing layers {layers_to_test[0]} to {layers_to_test[-1]}\")\n",
        "\n",
        "        results = []\n",
        "        total_calculations = len(pairs) * len(layers_to_test)\n",
        "\n",
        "        print(f\"Calculating effects for {len(pairs)} pairs and {len(layers_to_test)} layers\")\n",
        "        print(f\"Total calculations: {total_calculations}\")\n",
        "\n",
        "        with tqdm(total=total_calculations, desc=\"Calculating effects\") as pbar:\n",
        "            for pair_idx, pair in enumerate(pairs):\n",
        "                for layer_idx in layers_to_test:\n",
        "                    try:\n",
        "                        # Calculate effects\n",
        "                        effects = self.calculate_effects(pair, layer_idx)\n",
        "\n",
        "                        # Add pair metadata\n",
        "                        effects.update({\n",
        "                            'category': pair['category'],\n",
        "                            'intervention_position': pair['intervention_position'],\n",
        "                            'list_length': len(pair['original_list']),\n",
        "                            'original_word': pair['original_word'],\n",
        "                            'intervention_word': pair['intervention_word']\n",
        "                        })\n",
        "\n",
        "                        results.append(effects)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"\\nError with pair {pair['pair_id']}, layer {layer_idx}: {e}\")\n",
        "                        # Add failed result\n",
        "                        results.append({\n",
        "                            'pair_id': pair['pair_id'],\n",
        "                            'layer_idx': layer_idx,\n",
        "                            'error': str(e),\n",
        "                            'TE': np.nan,\n",
        "                            'IE': np.nan\n",
        "                        })\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "                # Save intermediate results\n",
        "                if (pair_idx + 1) % save_frequency == 0:\n",
        "                    df_temp = pd.DataFrame(results)\n",
        "                    temp_path = f\"{output_dir}/cma_effects_intermediate_{pair_idx+1}.csv\"\n",
        "                    df_temp.to_csv(temp_path, index=False)\n",
        "                    print(f\"\\nSaved intermediate results ({pair_idx+1} pairs processed)\")\n",
        "\n",
        "                # Clear cache periodically\n",
        "                if (pair_idx + 1) % 5 == 0:\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "        # Create final dataframe\n",
        "        results_df = pd.DataFrame(results)\n",
        "\n",
        "        # Save final results\n",
        "        final_path = f\"{output_dir}/cma_effects_results.csv\"\n",
        "        results_df.to_csv(final_path, index=False)\n",
        "        print(f\"\\nSaved final results to {final_path}\")\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def run_analysis(self, pairs_path: str = \"/content/counting_dataset/cma_intervention_pairs.json\",\n",
        "                    layers_to_test: Optional[List[int]] = None,\n",
        "                    max_pairs: Optional[int] = None,\n",
        "                    output_dir: str = \"/content/counting_dataset\") -> pd.DataFrame:\n",
        "        \"\"\"Complete pipeline to run effects calculation\"\"\"\n",
        "\n",
        "        # Load pairs\n",
        "        print(f\"Loading pairs from {pairs_path}\")\n",
        "        with open(pairs_path, 'r') as f:\n",
        "            pairs = json.load(f)\n",
        "\n",
        "        # Limit pairs if specified\n",
        "        if max_pairs:\n",
        "            pairs = pairs[:max_pairs]\n",
        "            print(f\"Using first {max_pairs} pairs\")\n",
        "\n",
        "        # Calculate effects\n",
        "        results_df = self.calculate_effects_batch(pairs, layers_to_test, output_dir=output_dir)\n",
        "\n",
        "        # Print summary statistics\n",
        "        self.print_summary_stats(results_df)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def print_summary_stats(self, results_df: pd.DataFrame):\n",
        "        \"\"\"Print summary statistics of the results\"\"\"\n",
        "        print(\"\\n=== Effects Calculation Summary ===\")\n",
        "        print(f\"Total calculations: {len(results_df)}\")\n",
        "\n",
        "        # Check for errors\n",
        "        if 'error' in results_df.columns:\n",
        "            error_count = results_df['error'].notna().sum()\n",
        "            print(f\"Errors: {error_count}\")\n",
        "\n",
        "        # Basic statistics\n",
        "        valid_results = results_df[results_df['TE'].notna()]\n",
        "        print(f\"Valid results: {len(valid_results)}\")\n",
        "\n",
        "        if len(valid_results) > 0:\n",
        "            print(\"\\nModel Accuracy:\")\n",
        "            orig_acc = valid_results['original_correct'].mean()\n",
        "            int_acc = valid_results['intervention_correct'].mean()\n",
        "            print(f\"  Original prompts: {orig_acc:.2%} ({valid_results['original_correct'].sum()}/{len(valid_results)})\")\n",
        "            print(f\"  Intervention prompts: {int_acc:.2%} ({valid_results['intervention_correct'].sum()}/{len(valid_results)})\")\n",
        "\n",
        "            print(\"\\nEffect Statistics:\")\n",
        "            print(f\"  Mean TE: {valid_results['TE'].mean():.3f} (std: {valid_results['TE'].std():.3f})\")\n",
        "            print(f\"  Mean |TE|: {np.abs(valid_results['TE']).mean():.3f}\")\n",
        "            print(f\"  Mean IE: {valid_results['IE'].mean():.3f} (std: {valid_results['IE'].std():.3f})\")\n",
        "\n",
        "            # Effect by layer\n",
        "            print(\"\\nEffects by Layer:\")\n",
        "            layer_effects = valid_results.groupby('layer_idx')[['TE', 'IE']].mean()\n",
        "            print(layer_effects.round(3))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize calculator\n",
        "    calculator = CMAEffectsCalculator(model_name=\"microsoft/phi-4\")\n",
        "\n",
        "    # Run analysis with specific output directory\n",
        "    results_df = calculator.run_analysis(\n",
        "        pairs_path=\"/content/CMA_analysis/cma_intervention_pairs.json\",\n",
        "        laye\n",
        "        rs_to_test=list(range(20)),\n",
        "        max_pairs=100,\n",
        "        output_dir=\"/content/CMA_analysis\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "    print(f\"Results shape: {results_df.shape}\")\n",
        "\n",
        "    # Show some example results\n",
        "    if len(results_df) > 0:\n",
        "        print(\"\\nExample results (first 3 rows):\")\n",
        "        cols_to_show = ['pair_id', 'layer_idx', 'TE', 'IE',\n",
        "                       'original_correct', 'intervention_correct', 'original_text', 'intervention_text']\n",
        "        print(results_df[cols_to_show].head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dc28870141164574baafd4efacd9cea5",
            "9b2eabe0b5594a7e87fab9d6ce270ea4",
            "6488063d7f6a4c689260f7a2fb0776c3",
            "a4ebe1718d1d4816ad323b223d56df37",
            "ca093a377c484b13b0f186908e874761",
            "7641e2a43e85499684f1b56de9470024",
            "518294421eeb426f9ea4e0235d4f8246",
            "d18d3afb76a64110ac62a1906f57a147",
            "5118fed731204e17b14dc40345e5bbc9",
            "e32d7273adf74d8ea476783ccb4cb56a",
            "d4674610e74949b4be3e8938ebcf15cf"
          ]
        },
        "id": "entAELf6G_dF",
        "outputId": "7fe710be-35f7-4034-e3d8-4c084b882c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model microsoft/phi-4...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc28870141164574baafd4efacd9cea5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded with 40 layers\n",
            "Device: cuda\n",
            "Loading pairs from /content/CMA_analysis/cma_intervention_pairs.json\n",
            "Using first 100 pairs\n",
            "Calculating effects for 100 pairs and 20 layers\n",
            "Total calculations: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  10%|█         | 200/2000 [02:27<21:52,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (10 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  20%|██        | 400/2000 [04:54<19:22,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (20 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  30%|███       | 600/2000 [07:21<16:56,  1.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (30 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  40%|████      | 800/2000 [09:48<14:33,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (40 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  50%|█████     | 1000/2000 [12:16<12:21,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (50 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  60%|██████    | 1200/2000 [14:42<09:52,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (60 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  70%|███████   | 1400/2000 [17:09<07:19,  1.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (70 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  80%|████████  | 1600/2000 [19:36<04:58,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (80 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects:  90%|█████████ | 1800/2000 [22:03<02:28,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (90 pairs processed)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating effects: 100%|██████████| 2000/2000 [24:31<00:00,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved intermediate results (100 pairs processed)\n",
            "\n",
            "Saved final results to /content/CMA_analysis/cma_effects_results.csv\n",
            "\n",
            "=== Effects Calculation Summary ===\n",
            "Total calculations: 2000\n",
            "Valid results: 2000\n",
            "\n",
            "Model Accuracy:\n",
            "  Original prompts: 83.00% (1660/2000)\n",
            "  Intervention prompts: 87.00% (1740/2000)\n",
            "\n",
            "Effect Statistics:\n",
            "  Mean TE: -0.880 (std: 0.407)\n",
            "  Mean |TE|: 0.900\n",
            "  Mean IE: -0.595 (std: 0.532)\n",
            "\n",
            "Effects by Layer:\n",
            "             TE    IE\n",
            "layer_idx            \n",
            "0         -0.88 -0.87\n",
            "1         -0.88 -0.87\n",
            "2         -0.88 -0.87\n",
            "3         -0.88 -0.87\n",
            "4         -0.88 -0.87\n",
            "5         -0.88 -0.87\n",
            "6         -0.88 -0.86\n",
            "7         -0.88 -0.86\n",
            "8         -0.88 -0.85\n",
            "9         -0.88 -0.84\n",
            "10        -0.88 -0.78\n",
            "11        -0.88 -0.70\n",
            "12        -0.88 -0.63\n",
            "13        -0.88 -0.38\n",
            "14        -0.88 -0.28\n",
            "15        -0.88 -0.22\n",
            "16        -0.88 -0.16\n",
            "17        -0.88 -0.09\n",
            "18        -0.88 -0.03\n",
            "19        -0.88  0.00\n",
            "\n",
            "Analysis complete!\n",
            "Results shape: (2000, 23)\n",
            "\n",
            "Example results (first 3 rows):\n",
            "   pair_id  layer_idx  TE  IE  original_correct  intervention_correct  \\\n",
            "0        0          0  -1  -1              True                  True   \n",
            "1        0          1  -1  -1              True                  True   \n",
            "2        0          2  -1  -1              True                  True   \n",
            "\n",
            "  original_text intervention_text  \n",
            "0           (3)               (2)  \n",
            "1           (3)               (2)  \n",
            "2           (3)               (2)  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### analysis"
      ],
      "metadata": {
        "id": "NI3A4-FLJs1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "import os\n",
        "\n",
        "class SimplifiedCMAAnalyzer:\n",
        "    def __init__(self, results_path: str = \"/content/counting_dataset/cma_effects_results.csv\",\n",
        "                 output_dir: str = \"/content/counting_dataset\"):\n",
        "        \"\"\"Initialize analyzer with results data\"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Loading results from {results_path}\")\n",
        "        self.results_df = pd.read_csv(results_path)\n",
        "        print(f\"Loaded {len(self.results_df)} results\")\n",
        "\n",
        "        # Clean data\n",
        "        self.results_df = self.results_df[self.results_df['TE'].notna()]\n",
        "        print(f\"Valid results: {len(self.results_df)}\")\n",
        "\n",
        "    def compute_layer_statistics(self) -> pd.DataFrame:\n",
        "        \"\"\"Compute statistics for each layer\"\"\"\n",
        "        layer_stats = []\n",
        "\n",
        "        for layer_idx in sorted(self.results_df['layer_idx'].unique()):\n",
        "            layer_data = self.results_df[self.results_df['layer_idx'] == layer_idx]\n",
        "\n",
        "            stats_dict = {\n",
        "                'layer_idx': layer_idx,\n",
        "                'n_samples': len(layer_data),\n",
        "                # Effect statistics\n",
        "                'TE_mean': layer_data['TE'].mean(),\n",
        "                'TE_std': layer_data['TE'].std(),\n",
        "                'TE_abs_mean': layer_data['TE'].abs().mean(),\n",
        "                'IE_mean': layer_data['IE'].mean(),\n",
        "                'IE_std': layer_data['IE'].std(),\n",
        "                'IE_abs_mean': layer_data['IE'].abs().mean(),\n",
        "                # Accuracy metrics\n",
        "                'correct_original': layer_data['original_correct'].mean() if 'original_correct' in layer_data.columns else 0,\n",
        "                'correct_intervention': layer_data['intervention_correct'].mean() if 'intervention_correct' in layer_data.columns else 0,\n",
        "            }\n",
        "\n",
        "            layer_stats.append(stats_dict)\n",
        "\n",
        "        return pd.DataFrame(layer_stats).sort_values('layer_idx')\n",
        "\n",
        "    def plot_effect_magnitudes(self, layer_stats: pd.DataFrame, save_path: Optional[str] = None):\n",
        "        \"\"\"Plot effect magnitudes by layer\"\"\"\n",
        "        if save_path is None:\n",
        "            save_path = os.path.join(self.output_dir, \"effect_magnitudes_by_layer.png\")\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        x = layer_stats['layer_idx']\n",
        "        plt.plot(x, layer_stats['TE_abs_mean'], 'o-', label='|TE| (Total Effect)',\n",
        "                markersize=8, linewidth=2, color='blue')\n",
        "        plt.plot(x, layer_stats['IE_abs_mean'], '^-', label='|IE| (Indirect Effect)',\n",
        "                markersize=8, linewidth=2, color='red')\n",
        "\n",
        "        plt.xlabel('Layer Index', fontsize=14)\n",
        "        plt.ylabel('Mean Absolute Effect', fontsize=14)\n",
        "        plt.title('Effect Magnitudes by Layer', fontsize=16, fontweight='bold')\n",
        "        plt.legend(fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Fix x-axis to show integer ticks\n",
        "        plt.xticks(x.astype(int))\n",
        "\n",
        "        # Add some styling\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved effect magnitudes plot to {save_path}\")\n",
        "\n",
        "    def calculate_overall_accuracy(self) -> Dict[str, float]:\n",
        "        \"\"\"Calculate overall correct rates\"\"\"\n",
        "        overall_stats = {}\n",
        "\n",
        "        if 'original_correct' in self.results_df.columns:\n",
        "            overall_stats['original_correct_rate'] = self.results_df['original_correct'].mean()\n",
        "        else:\n",
        "            overall_stats['original_correct_rate'] = 0\n",
        "\n",
        "        if 'intervention_correct' in self.results_df.columns:\n",
        "            overall_stats['intervention_correct_rate'] = self.results_df['intervention_correct'].mean()\n",
        "        else:\n",
        "            overall_stats['intervention_correct_rate'] = 0\n",
        "\n",
        "        return overall_stats\n",
        "\n",
        "    def plot_overall_accuracy(self, save_path: Optional[str] = None):\n",
        "        \"\"\"Plot overall correct rates\"\"\"\n",
        "        if save_path is None:\n",
        "            save_path = os.path.join(self.output_dir, \"overall_correct_rates.png\")\n",
        "\n",
        "        accuracy_stats = self.calculate_overall_accuracy()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        categories = ['Original', 'After Intervention']\n",
        "        rates = [accuracy_stats['original_correct_rate'], accuracy_stats['intervention_correct_rate']]\n",
        "        colors = ['skyblue', 'lightcoral']\n",
        "\n",
        "        bars = plt.bar(categories, rates, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, rate in zip(bars, rates):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                    f'{rate:.2%}', ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
        "\n",
        "        plt.ylabel('Correct Rate', fontsize=14)\n",
        "        plt.title('Overall Model Accuracy: Original vs After Intervention', fontsize=16, fontweight='bold')\n",
        "        plt.ylim(0, 1.1)\n",
        "        plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add percentage formatting to y-axis\n",
        "        plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved overall accuracy plot to {save_path}\")\n",
        "\n",
        "        return accuracy_stats\n",
        "\n",
        "    def run_analysis(self):\n",
        "        \"\"\"Run the simplified analysis with only the two required figures\"\"\"\n",
        "        print(\"Running simplified CMA analysis...\")\n",
        "\n",
        "        # Compute layer statistics\n",
        "        layer_stats = self.compute_layer_statistics()\n",
        "\n",
        "        # Save layer statistics\n",
        "        stats_path = os.path.join(self.output_dir, \"layer_statistics.csv\")\n",
        "        layer_stats.to_csv(stats_path, index=False)\n",
        "        print(f\"Saved layer statistics to {stats_path}\")\n",
        "\n",
        "        # Generate the two required plots\n",
        "        self.plot_effect_magnitudes(layer_stats)\n",
        "        accuracy_stats = self.plot_overall_accuracy()\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\n=== ANALYSIS SUMMARY ===\")\n",
        "        print(f\"Total samples analyzed: {len(self.results_df)}\")\n",
        "        print(f\"Number of layers: {len(layer_stats)}\")\n",
        "        print(f\"Layer range: {layer_stats['layer_idx'].min()} to {layer_stats['layer_idx'].max()}\")\n",
        "        print(f\"\\nOverall Accuracy:\")\n",
        "        print(f\"  Original correct rate: {accuracy_stats['original_correct_rate']:.2%}\")\n",
        "        print(f\"  After intervention correct rate: {accuracy_stats['intervention_correct_rate']:.2%}\")\n",
        "        print(f\"  Accuracy change: {accuracy_stats['intervention_correct_rate'] - accuracy_stats['original_correct_rate']:.2%}\")\n",
        "\n",
        "        print(f\"\\nEffect Statistics:\")\n",
        "        print(f\"  Mean |TE| across all layers: {self.results_df['TE'].abs().mean():.3f}\")\n",
        "        print(f\"  Mean |IE| across all layers: {self.results_df['IE'].abs().mean():.3f}\")\n",
        "\n",
        "        # Find layer with highest effects\n",
        "        max_te_layer = layer_stats.loc[layer_stats['TE_abs_mean'].idxmax()]\n",
        "        max_ie_layer = layer_stats.loc[layer_stats['IE_abs_mean'].idxmax()]\n",
        "\n",
        "        print(f\"\\nHighest Effects:\")\n",
        "        print(f\"  Highest |TE|: Layer {int(max_te_layer['layer_idx'])} ({max_te_layer['TE_abs_mean']:.3f})\")\n",
        "        print(f\"  Highest |IE|: Layer {int(max_ie_layer['layer_idx'])} ({max_ie_layer['IE_abs_mean']:.3f})\")\n",
        "\n",
        "        return layer_stats, accuracy_stats\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize analyzer\n",
        "    analyzer = SimplifiedCMAAnalyzer(\n",
        "        results_path=\"/content/CMA_analysis/cma_effects_results.csv\",\n",
        "        output_dir=\"/content/CMA_analysis\"\n",
        "    )\n",
        "\n",
        "    # Run analysis with only the two required figures\n",
        "    layer_stats, accuracy_stats = analyzer.run_analysis()\n",
        "\n",
        "    # Display top 5 layers by IE\n",
        "    print(\"\\n=== Top 5 Layers by Indirect Effect ===\")\n",
        "    top_ie_layers = layer_stats.nlargest(5, 'IE_abs_mean')\n",
        "    print(top_ie_layers[['layer_idx', 'IE_abs_mean', 'TE_abs_mean', 'correct_original', 'correct_intervention']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFaWKbyJJuF_",
        "outputId": "fa66f91f-dff3-4207-833d-9e4c31abed2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading results from /content/CMA_analysis/cma_effects_results.csv\n",
            "Loaded 2000 results\n",
            "Valid results: 2000\n",
            "Running simplified CMA analysis...\n",
            "Saved layer statistics to /content/CMA_analysis/layer_statistics.csv\n",
            "Saved effect magnitudes plot to /content/CMA_analysis/effect_magnitudes_by_layer.png\n",
            "Saved overall accuracy plot to /content/CMA_analysis/overall_correct_rates.png\n",
            "\n",
            "=== ANALYSIS SUMMARY ===\n",
            "Total samples analyzed: 2000\n",
            "Number of layers: 20\n",
            "Layer range: 0 to 19\n",
            "\n",
            "Overall Accuracy:\n",
            "  Original correct rate: 83.00%\n",
            "  After intervention correct rate: 87.00%\n",
            "  Accuracy change: 4.00%\n",
            "\n",
            "Effect Statistics:\n",
            "  Mean |TE| across all layers: 0.900\n",
            "  Mean |IE| across all layers: 0.623\n",
            "\n",
            "Highest Effects:\n",
            "  Highest |TE|: Layer 0 (0.900)\n",
            "  Highest |IE|: Layer 7 (0.900)\n",
            "\n",
            "=== Top 5 Layers by Indirect Effect ===\n",
            "   layer_idx  IE_abs_mean  TE_abs_mean  correct_original  correct_intervention\n",
            "7          7         0.90          0.9              0.83                  0.87\n",
            "0          0         0.89          0.9              0.83                  0.87\n",
            "1          1         0.89          0.9              0.83                  0.87\n",
            "2          2         0.89          0.9              0.83                  0.87\n",
            "3          3         0.89          0.9              0.83                  0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save result"
      ],
      "metadata": {
        "id": "WXZ4jUNLRyCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src_dir = '/content/CMA_analysis'\n",
        "dst_dir = '/content/drive/MyDrive/counting_project/counting_dataset/CMA_analysis'\n",
        "\n",
        "# Make sure destination directory exists\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Loop through all files in source directory\n",
        "for filename in os.listdir(src_dir):\n",
        "    src_file = os.path.join(src_dir, filename)\n",
        "    dst_file = os.path.join(dst_dir, filename)\n",
        "\n",
        "    # Only copy files (skip subdirectories)\n",
        "    if os.path.isfile(src_file):\n",
        "        shutil.copy2(src_file, dst_file)  # copy2 preserves metadata"
      ],
      "metadata": {
        "id": "qGPdy9mfR6H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "9l7fbf0pqWUL"
      }
    }
  ]
}